- **Date of Entry:** 2026-01-31  
- **Tags:** #statistics #estimation  


(Consistency ‚Ä¢ Efficiency ‚Ä¢ Sufficiency)

## One-line idea
A good estimator should converge to the truth, use information efficiently, and retain all relevant information from the data.

---

## The three fundamental properties

Classical estimation theory evaluates estimators using three core properties:

1. **Consistency**  
2. **Efficiency**  
3. **Sufficiency**

Each answers a different question.

---

## Conceptual comparison

| Property | Core Question | What it Measures |
|--------|----------------|------------------|
| Consistency | Does it converge to the true parameter? | Long-run accuracy |
| Efficiency | How precise is it compared to others? | Variance |
| Sufficiency | Does it use all information in the data? | Information retention |

---

## 1Ô∏è‚É£ Consistency

An estimator is **consistent** if:

- as sample size increases  
- the estimate approaches the true parameter  

More data ‚Üí better estimate.

Consistency concerns **asymptotic behaviour**.

---

## 2Ô∏è‚É£ Efficiency

Among unbiased estimators:

- the one with the smallest variance is most efficient  

Efficiency determines:

- precision  
- narrow confidence intervals  
- optimal use of data  

It is a relative concept.

---

## 3Ô∏è‚É£ Sufficiency

A statistic is sufficient if:

- it captures all information in the sample about the parameter  
- no additional data improves estimation  

Once known, raw data is unnecessary.

---

## Key differences

| Aspect | Consistency | Efficiency | Sufficiency |
|------|-------------|-------------|-------------|
| Focus | Convergence | Precision | Information |
| Depends on sample size | Yes | Yes | No |
| Long-run concept | Yes | No | No |
| Variance-related | Indirect | Direct | Indirect |
| Data compression | No | No | Yes |

---

## Important insight

An estimator can be:

- consistent but inefficient  
- efficient but not sufficient  
- sufficient but biased  

No single property guarantees optimal estimation alone.

---

## In short

> Good estimation requires convergence, precision, and full use of information.

---

# Common Estimators and Their Properties

## One-line idea
Different estimators possess different combinations of consistency, efficiency, and sufficiency depending on how they use sample information.

---

## Estimator Comparison Table


| Estimator                              | Formula                                    | Consistent | Efficient    | Sufficient   | Why                                                                            |
| -------------------------------------- | ------------------------------------------ | ---------- | ------------ | ------------ | ------------------------------------------------------------------------------ |
| **Sample Mean**                        | $\bar{x} = \frac{1}{n}\sum x_i$<br>        | ‚úÖ Yes      | ‚úÖ Yes*       | ‚úÖ Yes*       | Converges by LLN; minimum variance under normality; sufficient for Œº in normal |
| **Sample Median**                      | middle value                               | ‚úÖ Yes      | ‚ùå No         | ‚ùå No         | Converges to median but has higher variance                                    |
| **Sample Variance (s¬≤)**               | $( \frac{1}{n-1}\sum(x_i-\bar{x})^2 )$<br> | ‚úÖ Yes      | ‚ùå No         | ‚úÖ Yes*       | Consistent; not minimum variance; sufficient with mean in normal               |
| **Sample Proportion**                  | $( \hat{p} = \frac{X}{n} )$<br>            | ‚úÖ Yes      | ‚úÖ Yes*       | ‚úÖ Yes        | MLE for Bernoulli; attains CRLB                                                |
| **Method of Moments Estimator**        | equate moments                             | ‚úÖ Yes      | ‚ùå Usually no | ‚ùå Usually no | Simple but inefficient                                                         |
| **Maximum Likelihood Estimator (MLE)** | maximize likelihood                        | ‚úÖ Yes      | ‚úÖ Yes        | ‚úÖ Often      | Asymptotically efficient and sufficient                                        |
| **Unbiased Variance Estimator**        | \( s^2 \)                                  | ‚úÖ Yes      | ‚ùå No         | ‚ùå No         | Trades variance for unbiasedness                                               |
| **Biased Variance Estimator**          | $( \frac{1}{n}\sum(x_i-\bar{x})^2 )$       | ‚úÖ Yes      | ‚úÖ Yes        | ‚ùå No         | Lower variance but biased                                                      |
| **Trimmed Mean**                       | drop extremes                              | ‚úÖ Yes      | ‚ùå No         | ‚ùå No         | Robust but loses efficiency                                                    |
| **Sample Maximum**                     | max(x)                                     | ‚ùå No       | ‚ùå No         | ‚ùå No         | Does not converge to mean                                                      |

\* under normal-distribution assumptions

---

## Key insights

### ‚úÖ Consistency
- Most reasonable estimators are consistent.
- Consistency depends on **LLN**.

---

### ‚öñÔ∏è Efficiency
- Efficiency depends on estimator variance.
- MLEs are asymptotically efficient.
- Median is less efficient than mean.

---

### üì¶ Sufficiency
- Rare property.
- Holds mainly for exponential-family distributions.
- Normal distribution is special.

---

## Important patterns

| Pattern | Observation |
|------|------|
| Mean vs Median | Mean is efficient; median is robust |
| MOM vs MLE | MOM is simple; MLE is optimal |
| Unbiased vs Biased | Slight bias can reduce variance |
| Sufficiency | Mostly occurs in normal models |

---

## Master insight

> There is no universally best estimator ‚Äî only estimators optimized for different goals.

---

## One-line memory anchor
Consistency ‚Üí eventually correct  
Efficiency ‚Üí most precise  
Sufficiency ‚Üí no information wasted
