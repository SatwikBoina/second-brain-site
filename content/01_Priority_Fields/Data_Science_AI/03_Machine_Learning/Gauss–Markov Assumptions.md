---
type: concept
domain: machine-learning
category: regression-theory
status: evergreen
---

# Gaussâ€“Markov Assumptions

> The Gaussâ€“Markov assumptions guarantee that the Ordinary Least Squares (OLS) estimator is the Best Linear Unbiased Estimator (BLUE).

---

# 1. One-Line Intuition

> If certain assumptions hold, OLS gives the lowest-variance unbiased linear estimator.

---

# 2. The Linear Model

We assume:

$$

y = X\beta + \epsilon 
$$
OLS estimator:
$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$


Where:

- \(y\) = target vector
- \(X\) = design matrix
- \(\beta\) = coefficients
- \(\epsilon\) = error term

---

# 3. The Gaussâ€“Markov Assumptions

There are five key assumptions.

---

## 3.1 Linearity in Parameters

The model is linear in coefficients:

$$
y = X\beta + \epsilon
$$
Not necessarily linear in features.

ðŸ”— Related:
- [[Linearity Assumption]]

---

## 3.2 Random Sampling (IID Observations)

Observations are independently and identically distributed.

No systematic dependence.

ðŸ”— Related:
- [[Independence of Errors]]

---

## 3.3 No Perfect Multicollinearity

Features are not perfectly linearly dependent.

$$
rank(X) = p
$$

Matrix \(X^T X\) must be invertible.

ðŸ”— Related:
- [[Multicollinearity]]

---

## 3.4 Zero Conditional Mean

$$
E[\epsilon | X] = 0
$$

Errors are uncorrelated with predictors.

This ensures unbiasedness.

---

## 3.5 Homoscedasticity

$$
Var(\epsilon | X) = \sigma^2
$$

Constant variance of errors.

ðŸ”— Related:
- [[Homoscedasticity]]

---

# 4. What the Gaussâ€“Markov Theorem States

If assumptions 1â€“5 hold:

$$
\hat{\beta}_{OLS}
$$

is the:

> **Best Linear Unbiased Estimator (BLUE)**

Meaning:

- Linear estimator
- Unbiased
- Minimum variance among linear unbiased estimators

---

# 5. What â€œBestâ€ Means

Best = Minimum Variance

Among all linear unbiased estimators:

$$
Var(\hat{\beta}_{OLS}) \le Var(\tilde{\beta})
$$

for any other linear unbiased estimator.

---

# 6. Important Clarification

Gaussâ€“Markov does NOT require:

- Normality of errors

Normality is only needed for:

- Exact inference
- Hypothesis testing
- t-tests and F-tests

ðŸ”— Related:
- [[Normality of Errors]]

---

# 7. What Happens If Assumptions Fail?

---

## Violate Linearity

â†’ Model misspecified  
â†’ Biased estimates  

---

## Violate Zero Conditional Mean

â†’ Omitted variable bias  
â†’ Endogeneity  

---

## Violate Homoscedasticity

â†’ OLS still unbiased  
â†’ Not minimum variance  
â†’ Use robust standard errors  

---

## Multicollinearity

â†’ High variance  
â†’ Unstable coefficients  

---

# 8. Geometric Interpretation

OLS projects:

\[
y
\]

onto the column space of:

\[
X
\]

Gaussâ€“Markov guarantees:

- This projection has minimum variance among linear unbiased projections.

---

# 9. Gaussâ€“Markov vs Maximum Likelihood

If errors are normal:

- OLS = Maximum Likelihood Estimator (MLE)

Without normality:

- OLS still BLUE
- But not necessarily MLE

ðŸ”— Related:
- [[Maximum Likelihood Estimation]]

---

# 10. Biasâ€“Variance Perspective

Gaussâ€“Markov is fundamentally about:

- Variance minimization
- Under unbiased constraint

It is a variance-optimality theorem.

ðŸ”— Related:
- [[Biasâ€“Variance Tradeoff]]

---

# 11. Why It Matters in ML

In pure prediction tasks:

- Violations may not matter much.

In inference tasks:

- Assumptions are critical.
- Confidence intervals depend on them.

---

# 12. Relationship to Other Concepts

Gaussâ€“Markov connects strongly to:

- [[Linear Regression]]
- [[Linearity Assumption]]
- [[Independence of Errors]]
- [[Homoscedasticity]]
- [[Multicollinearity]]
- [[Maximum Likelihood Estimation]]
- [[Biasâ€“Variance Tradeoff]]

---

# 13. Why Gaussâ€“Markov Matters

It teaches:

- Why OLS works
- What makes estimators optimal
- Difference between unbiasedness and efficiency
- Why regression assumptions exist

It is the theoretical foundation of:
> Classical linear regression.
