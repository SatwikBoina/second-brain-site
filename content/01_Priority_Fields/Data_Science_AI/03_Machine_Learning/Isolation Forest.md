---
type: algorithm
domain: machine-learning
category: anomaly-detection
model_family: tree-based
supervision: unsupervised
primary_use: anomaly_detection
status: evergreen
---

# Isolation Forest

> Isolation Forest is an anomaly detection algorithm that isolates anomalies instead of profiling normal data.

---

## 1. One-Line Intuition

> Anomalies are easier to isolate than normal points.

Because:
- They are rare.
- They are far from dense regions.

So fewer splits are needed to isolate them.

---

## 2. Why Isolation Forest Exists

Traditional anomaly detection:

- Distance-based (KNN)
- Density-based (KDE, LOF)
- Probabilistic (GMM)

Problems:
- Expensive in high dimensions
- Sensitive to scaling
- Struggle with large datasets

Isolation Forest:
- Uses random trees
- Does not rely on distance
- Scales well

ðŸ”— Related:
- [[Outliers]]
- [[Distance-Based Models â€” MOC]]

---

## 3. Core Idea

Isolation Forest builds many random binary trees.

Each tree:

1. Randomly selects a feature
2. Randomly selects a split value
3. Recursively splits until point is isolated

Anomalies:
- Require fewer splits
- Have shorter path lengths

---

## 4. Isolation Process

For a dataset:

- Subsample data
- Build many isolation trees
- Compute average path length for each point

Anomaly score:

\[
Score(x) \propto 2^{-\frac{E(h(x))}{c(n)}}
\]

Where:
- \(E(h(x))\) = average path length
- \(c(n)\) = normalization factor

Shorter path â†’ higher anomaly score.

---

## 5. Geometric Interpretation

Normal points:
- Deep inside clusters
- Require many splits

Anomalies:
- Isolated early
- Few splits needed

Isolation Forest exploits data sparsity.

---

## 6. Key Hyperparameters

---

### 6.1 Number of Trees

More trees:
- More stable estimates
- Higher computation

---

### 6.2 Subsample Size

Small subsample:
- Faster
- Better anomaly contrast

Typical:
- 256 samples per tree

---

### 6.3 Contamination

Expected proportion of anomalies.

Used to:
- Set threshold

ðŸ”— Related:
- [[Hyperparameter Tuning â€” MOC]]

---

## 7. Strengths

- Scales to large datasets
- Works well in high dimensions
- No distance computation
- Robust to scaling issues
- Efficient

---

## 8. Weaknesses

- Random splits may miss structure
- Not ideal for very subtle anomalies
- Sensitive to contamination parameter

---

## 9. Failure Modes

- âŒ Dense anomalies (clustered anomalies)
- âŒ Very small datasets
- âŒ Poor subsampling
- âŒ Complex structured anomalies

Isolation works best for:
> Sparse, distinct anomalies.

---

## 10. Isolation Forest vs Other Methods

| Method | Based On | High-Dim Robust | Distance Needed | Noise Detection |
|----------|----------|----------------|----------------|----------------|
| KNN | Distance | No | Yes | Yes |
| LOF | Density | Moderate | Yes | Yes |
| KDE | Density | No | Yes | Yes |
| GMM | Probability | Moderate | Yes | Yes |
| Isolation Forest | Isolation | Yes | No | Yes |

Isolation Forest avoids:
- Distance concentration problem.

ðŸ”— Related:
- [[Curse of Dimensionality]]

---

## 11. Computational Complexity

Training:

\[
O(t \cdot \psi \log \psi)
\]

Where:
- t = number of trees
- Ïˆ = subsample size

Very scalable.

---

## 12. Isolation Forest and Trees

Isolation Forest is similar to:

- Random Forest
- Extremely Randomized Trees

But:
- No target variable
- No impurity measure
- Purely random splits

ðŸ”— Related:
- [[Random Forest Classifier]]
- [[Decision Tree Classifier]]

---

## 13. Use Cases

- Fraud detection
- Intrusion detection
- Industrial sensor monitoring
- Healthcare anomaly detection
- Log anomaly detection

---

## 14. When Isolation Forest Works Well

- Large datasets
- High-dimensional data
- Sparse anomalies
- Unknown distribution

---

## 15. When Itâ€™s a Bad Idea

- Structured anomaly patterns
- Highly clustered anomalies
- Very small datasets

---

## 16. Relationship to Other Concepts

Isolation Forest connects strongly to:

- [[Outliers]]
- [[Random Forest Classifier]]
- [[Decision Tree Classifier]]
- [[Curse of Dimensionality]]
- [[Anomaly Detection â€” MOC]] (if created later)

---

## 17. Why Isolation Forest Matters

Isolation Forest teaches:

- Tree-based anomaly detection
- Why anomalies are sparse
- How randomness can be powerful
- Why distance isnâ€™t always necessary

It is one of the most practical anomaly detection algorithms.
