---
type: algorithm
domain: machine-learning
paradigm: supervised
problem-type: regression
model-family: kernel-methods
status: evergreen
---

# Support Vector Regression (SVR)

> A kernel-based regression algorithm that fits a function within a specified error margin while maximizing model flatness.

---

## 1. Core Idea

Support Vector Regression extends Support Vector Machines to regression by:

- Allowing a margin of error (Îµ) around predictions
- Penalizing predictions only when errors exceed this margin
- Maximizing model flatness instead of minimizing squared error

ðŸ”— Model family:
- [[Kernel Methods â€” MOC]]

---

## 2. Intuition

SVR tries to fit a function such that:
- Most data points lie **inside a tolerance tube**
- Only points outside the tube influence the model
- The model remains as simple (flat) as possible

---

## 3. Analogy

Imagine fitting a **road** through a city:
- The road has a fixed width (Îµ)
- Buildings inside the road donâ€™t matter
- Only buildings sticking out force the road to bend

---

## 4. Mathematical Perspective

SVR solves an optimization problem that:

- Minimizes model complexity (â€–wâ€–Â²)
- Allows Îµ-insensitive errors
- Uses slack variables for violations

ðŸ”— Concepts:
- [[Epsilon-Insensitive Loss]]
- [[Convex Optimization]]
- [[Lagrangian Duality]]

---

## 5. Loss Function

SVR uses **Îµ-insensitive loss**:

- Errors â‰¤ Îµ â†’ ignored
- Errors > Îµ â†’ penalized linearly

ðŸ”— See:
- [[Loss Functions â€” MOC]]

---

## 6. Kernel Trick

SVR uses kernels to model non-linear relationships:

- Linear
- Polynomial
- RBF (Gaussian)
- Sigmoid

ðŸ”— Concepts:
- [[Kernel Trick]]
- [[Reproducing Kernel Hilbert Space]]

---

## 7. Support Vectors

- Only a subset of points define the model
- Points outside the Îµ-tube become support vectors
- Sparse solution (depends on Îµ and C)

ðŸ”— See:
- [[Support Vectors]]

---

## 8. Biasâ€“Variance Tradeoff

- High bias with large Îµ
- High variance with small Îµ and large C
- Strongly controlled by hyperparameters

ðŸ”— Links:
- [[Biasâ€“Variance Tradeoff]]

---

## 9. Assumptions

SVR assumes:

- Data can be modeled in some feature space
- Kernel choice reflects similarity structure
- Sensitive to noise and scaling

ðŸ”— Contrast:
- [[Tree-Based Models â€” MOC]]
- [[Linear Models â€” MOC]]

---

## 10. Hyperparameters (Critical)

Key hyperparameters include:

- C (regularization strength)
- Îµ (epsilon margin)
- Kernel type
- Kernel-specific parameters (gamma, degree)

ðŸ”— See:
- [[Hyperparameter Tuning â€” MOC]]

---

## 11. Feature Scaling Requirement

SVR is **highly sensitive to feature scale**:

- Feature standardization is mandatory
- Kernel distances depend on scale

ðŸ”— Links:
- [[Feature Scaling]]
- [[Standardization]]

---

## 12. Evaluation Metrics

Uses standard regression metrics:

- RMSE
- MAE
- RÂ²

ðŸ”— See:
- [[Regression Metrics â€” MOC]]

---

## 13. Interpretability

- Linear SVR â†’ interpretable coefficients
- Kernel SVR â†’ low interpretability
- Support vectors give limited insight

ðŸ”— Links:
- [[Model Interpretability]]

---

## 14. Computational Characteristics

- Training scales poorly with dataset size
- Memory-intensive for large datasets
- Not ideal for very large tabular data

ðŸ”— Concepts:
- [[Computational Complexity]]

---

## 15. When SVR Works Well

- Small to medium datasets
- Smooth non-linear relationships
- High-dimensional feature spaces
- When trees overfit

---

## 16. When It Fails

- Large datasets
- Noisy data
- Poor kernel choice
- Improper feature scaling

---

## 17. Relationship to Other Models

- Regression counterpart of SVM
- Kernel-based alternative to polynomial regression

ðŸ”— Related:
- [[Support Vector Machine]]
- [[Polynomial Regression]]
- [[Kernel Ridge Regression]]

---

## 18. Position in the ML Landscape

- Powerful but computationally expensive
- Strong theoretical foundation
- Less common in large-scale production systems
