---
type: algorithm
domain: machine-learning
category: unsupervised-learning
model_family: probabilistic
primary_use: clustering
status: evergreen
---

# Gaussian Mixture Models (GMM)

> A Gaussian Mixture Model represents data as a mixture of multiple Gaussian distributions and assigns each point a probability of belonging to each cluster.

---

## 1. One-Line Intuition

> Instead of assigning a point to the nearest centroid (like K-Means), GMM estimates the probability that the point was generated by each Gaussian component.

---

## 2. Core Idea

Assume data is generated from:

\[
p(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)
\]

Where:
- \( \pi_k \) = mixing weight (cluster prior)
- \( \mu_k \) = mean
- \( \Sigma_k \) = covariance matrix
- K = number of components

Each cluster is a Gaussian distribution.

---

## 3. Hard vs Soft Clustering

K-Means:
- Hard assignment
- Each point belongs to one cluster

GMM:
- Soft assignment
- Each point has probabilities over clusters

Example:

Point A:
- 70% Cluster 1
- 30% Cluster 2

---

## 4. Expectation‚ÄìMaximization (EM) Algorithm

GMM parameters are estimated using EM.

---

### Step 1: Initialize Parameters

- Means
- Covariances
- Mixing weights

Often initialized using K-Means.

---

### Step 2: E-Step (Expectation)

Compute responsibility:

\[
\gamma(z_{ik}) = P(z_k \mid x_i)
\]

Probability that point \(x_i\) belongs to cluster k.

---

### Step 3: M-Step (Maximization)

Update:

- Means
- Covariances
- Mixing weights

Using weighted averages.

---

Repeat until:
- Log-likelihood converges.

---

## 5. Geometric Interpretation

K-Means:
- Clusters are spherical.
- Equal variance.
- Centroid-based.

GMM:
- Clusters can be elliptical.
- Each cluster has its own covariance.
- More flexible shapes.

---

## 6. Covariance Types

---

### 6.1 Full Covariance

- Each cluster has its own covariance matrix.
- Most flexible.
- Most parameters.

---

### 6.2 Diagonal Covariance

- Only variances, no correlations.
- Simpler.

---

### 6.3 Spherical

- Equal variance in all directions.
- Similar to K-Means.

---

### 6.4 Tied Covariance

- All clusters share one covariance matrix.

---

## 7. Objective Function

GMM maximizes:

\[
\log p(X)
\]

Log-likelihood of the data.

Unlike K-Means:
- GMM is probabilistic.
- Optimizes likelihood, not distance.

üîó Related:
- [[Probabilistic Models ‚Äî MOC]]
- [[Optimization Methods ‚Äî MOC]]

---

## 8. Choosing Number of Components

Common approaches:

- BIC (Bayesian Information Criterion)
- AIC
- Cross-validation
- Domain knowledge

üîó Related:
- [[Model Selection]]

---

## 9. Strengths

- Soft clustering
- Flexible cluster shapes
- Probabilistic interpretation
- Captures covariance structure

---

## 10. Weaknesses

- Must choose K
- Sensitive to initialization
- Can converge to local optima
- Struggles in high dimensions
- Sensitive to singular covariance

---

## 11. Failure Modes

- ‚ùå Too many components ‚Üí overfitting
- ‚ùå Too few components ‚Üí underfitting
- ‚ùå Poor initialization
- ‚ùå Ill-conditioned covariance matrix
- ‚ùå High-dimensional sparse data

Regularization may be required.

---

## 12. GMM vs K-Means

| Aspect | K-Means | GMM |
|----------|----------|------|
| Assignment | Hard | Soft |
| Objective | Minimize WCSS | Maximize likelihood |
| Cluster shape | Spherical | Elliptical |
| Probabilistic | No | Yes |
| Covariance modeling | No | Yes |

K-Means is a special case of GMM with:
- Equal spherical covariance.

---

## 13. GMM and Density Estimation

GMM is not only clustering.

It can:
- Estimate full probability density
- Generate new samples
- Model multi-modal distributions

---

## 14. GMM and Anomaly Detection

Points with:
- Low likelihood under mixture
‚Üí Possible anomalies.

üîó Related:
- [[Outliers]]

---

## 15. GMM and Curse of Dimensionality

Covariance estimation requires:

\[
O(d^2)
\]

parameters per cluster.

High dimension ‚Üí unstable covariance.

Dimensionality reduction often required.

üîó Related:
- [[Principal Component Analysis]]
- [[Curse of Dimensionality]]

---

## 16. Relationship to Other Concepts

GMM connects strongly to:

- [[Probabilistic Models ‚Äî MOC]]
- [[K-Means]]
- [[Clustering Evaluation Metrics]]
- [[Optimization Methods ‚Äî MOC]]
- [[Model Selection]]
- [[Distance-Based Models ‚Äî MOC]]

---

## 17. When GMM Works Well

- Elliptical clusters
- Overlapping clusters
- Need soft assignment
- Moderate dimensionality

---

## 18. When GMM Is a Bad Idea

- Very high-dimensional data
- Heavy-tailed distributions
- Non-Gaussian clusters
- Very large datasets (computational cost)

---

## 19. Why GMM Matters

GMM teaches:

- Generative modeling
- Probabilistic clustering
- EM optimization
- Relationship between geometry and probability

It bridges:
- Distance-based clustering
- Bayesian modeling
- Latent variable models
