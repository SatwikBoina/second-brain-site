# ğŸ“Œ MOC â€” Loss Functions

> Central hub for **Loss Functions in Machine Learning**.
> A loss function defines **what it means to be wrong** and guides how a model learns by translating errors into a numerical signal optimized during training.

---

## 1. Core Idea

A loss function answers one question:

> *How bad is a prediction?*

It converts:
- True outcome \(y\)
- Predicted outcome \(\hat{y}\)

into a **scalar penalty** that optimization algorithms try to minimize.

Loss functions define:
- Learning behavior
- Model priorities
- Sensitivity to errors
- Robustness and fairness

---

## 2. Loss Functions vs Evaluation Metrics

Important distinction:

| Aspect | Loss Function | Evaluation Metric |
|----|----|----|
| Used during training | âœ… | âŒ |
| Differentiable (usually) | âœ… | âŒ |
| Optimized directly | âœ… | âŒ |
| Business-facing | âŒ | âœ… |

Loss â‰  Metric (by design).

ğŸ”— Related:
- [[Evaluation Metrics â€” MOC]]

---

## 3. Why Loss Functions Matter So Much

Loss functions determine:
- Which errors matter more
- How outliers are treated
- Whether probabilities are meaningful
- How biasâ€“variance tradeoff is navigated
- Whether the model aligns with business goals

> Changing the loss often changes the *entire behavior* of the model.

---

## 4. Major Families of Loss Functions

---

## 4.1 Regression Loss Functions

Used when targets are **continuous**.

Examples:
- Mean Squared Error
- Mean Absolute Error
- Huber Loss
- RMSLE
- Quantile Loss

ğŸ”— Hub:
- [[Regression Loss Functions â€” MOC]]

---

## 4.2 Classification Loss Functions

Used when targets are **categorical**.

Examples:
- Log Loss / Cross Entropy
- Hinge Loss
- Focal Loss
- Weighted Losses

ğŸ”— Hub:
- [[Classification Loss Functions â€” MOC]]

---

## 4.3 Margin-Based Losses

Focus on **decision boundaries**, not probabilities.

Examples:
- Hinge Loss
- Squared Hinge Loss

Used heavily in SVMs.

ğŸ”— Related:
- [[Margin-Based Losses]]

---

## 4.4 Probabilistic (Likelihood-Based) Losses

Derived from **negative log-likelihood**.

Examples:
- MSE â†” Gaussian likelihood
- Log loss â†” Bernoulli / Categorical likelihood

ğŸ”— Related:
- [[Probabilistic Models â€” MOC]]

---

## 4.5 Robust Loss Functions

Designed to handle:
- Outliers
- Noisy labels

Examples:
- MAE
- Huber Loss
- Log-Cosh Loss

ğŸ”— Related:
- [[Outliers]]

---

## 4.6 Asymmetric / Cost-Sensitive Losses

Penalize different mistakes differently.

Examples:
- Weighted Cross Entropy
- Quantile Loss
- Custom asymmetric losses

ğŸ”— Related:
- [[Cost-Sensitive Learning]]
- [[Custom Loss Functions]]

---

## 4.7 Custom Loss Functions

User-defined objectives to reflect:
- Business cost
- Risk
- Constraints

ğŸ”— Related:
- [[Custom Loss Functions]]

---

## 5. Loss Functions and Biasâ€“Variance

Loss choice affects biasâ€“variance behavior:

- L2-type losses â†’ lower bias, higher variance
- L1-type losses â†’ higher bias, lower variance
- Hybrid losses â†’ controlled tradeoff

ğŸ”— Related:
- [[Biasâ€“Variance Tradeoff]]

---

## 6. Loss Functions and Outliers

Outlier sensitivity depends heavily on loss shape:

| Loss | Outlier Sensitivity |
|----|---------------------|
| MSE | Very High |
| MAE | Low |
| Huber | Medium |
| Log-based | Low |

ğŸ”— Related:
- [[Outliers]]

---

## 7. Loss Functions and Optimization

Loss functions must be:
- Differentiable (or sub-differentiable)
- Smooth enough for gradient descent
- Numerically stable

Poorly designed losses:
- Break training
- Cause exploding gradients
- Lead to unstable convergence

ğŸ”— Related:
- [[Gradient Descent]]
- [[Optimization Algorithms]]

---

## 8. Loss Functions and Probability

Some losses produce **calibrated probabilities**, others do not.

- Log loss â†’ good calibration
- Hinge loss â†’ poor calibration

Probability quality matters for:
- Threshold tuning
- Cost-sensitive decisions
- Fairness evaluation

ğŸ”— Related:
- [[Probability Calibration]]
- [[Threshold Tuning]]

---

## 9. Loss Functions and Class Imbalance

Standard losses assume balanced data.

Imbalanced data requires:
- Reweighting
- Specialized losses
- Threshold tuning

ğŸ”— Related:
- [[Class Imbalance]]

---

## 10. Loss Functions and Model Choice

Different models naturally pair with different losses:

| Model | Typical Loss |
|----|-------------|
| Linear Regression | MSE |
| Logistic Regression | Log Loss |
| SVM | Hinge Loss |
| Gradient Boosting | Custom |
| Neural Networks | Cross Entropy / Custom |

Loss choice is **part of model design**.

---

## 11. Loss Functions and Interpretability

Loss functions influence:
- Coefficient meaning
- Feature importance
- Explanation stability

Robust losses often improve interpretability.

ğŸ”— Related:
- [[Model Interpretability]]

---

## 12. Common Mistakes

- âŒ Treating accuracy as a loss  
- âŒ Using MSE with heavy outliers  
- âŒ Ignoring business costs  
- âŒ Optimizing loss without checking metrics  
- âŒ Blindly using defaults  

Loss functions encode **values**, not just math.

---

## 13. How Loss Functions Fit in ML

Data  
â†“  
Features  
â†“  
Model  
â†“  
Loss Function â† defines learning  
â†“  
Optimization  
â†“  
Predictions


Everything downstream is shaped by the loss.

---

## Usage Rules

- This MOC is a conceptual + navigation hub
- Individual losses live in atomic notes
- Always link models to the loss they optimize
- Ask: *what behavior does this loss encourage?*
