| Metric                        | Formula                        | Denominator Focus      | What It Answers                                         | Penalizes More          | When To Use                                        |
| ----------------------------- | ------------------------------ | ---------------------- | ------------------------------------------------------- | ----------------------- | -------------------------------------------------- |
| **Accuracy**                  | (TP + TN) / Total              | All predictions        | Overall, how often am I correct?                        | Both equally            | Balanced dataset                                   |
| **Precision**                 | TP / (TP + FP)                 | Predicted Positive     | When I predict positive, how often am I right?          | False Positives         | When FP is costly (spam filter, wrongful approval) |
| **Recall (Sensitivity, TPR)** | TP / (TP + FN)                 | Actual Positive        | Of all real positives, how many did I catch?            | False Negatives         | When missing cases is costly (fraud, disease)      |
| **Specificity (TNR)**         | TN / (TN + FP)                 | Actual Negative        | Of all real negatives, how many did I reject correctly? | False Positives         | When false alarms are costly                       |
| **F1 Score**                  | 2PR / (P + R)                  | Balance of P & R       | How well do I balance precision & recall?               | Imbalance between P & R | When both FP & FN matter                           |
| **FPR**                       | FP / (FP + TN)                 | Actual Negative        | Of all negatives, how often did I falsely flag?         | False Positives         | Used in ROC                                        |
| **Balanced Accuracy**         | (Recall + Specificity)/2       | Both classes equally   | Average performance across classes                      | Class imbalance bias    | Imbalanced dataset                                 |
| **ROC-AUC**                   | Area under TPR vs FPR          | Ranking ability        | Can model rank positives above negatives?               | Poor ranking            | Balanced data or general evaluation                |
| **PR-AUC**                    | Area under Precision vs Recall | Positive class quality | How good is model at detecting positives?               | Minority class mistakes | Highly imbalanced data                             |