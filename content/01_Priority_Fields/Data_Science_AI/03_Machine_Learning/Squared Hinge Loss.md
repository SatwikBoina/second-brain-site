---
type: concept
domain: machine-learning
category: margin-based-learning
status: evergreen
---

# Squared Hinge Loss

> Squared Hinge Loss is a variation of hinge loss that penalizes margin violations quadratically instead of linearly.

---

## 1. One-Line Intuition

> If you violate the margin, you get penalized more severely â€” and the penalty grows quadratically.

---

## 2. Mathematical Definition

For binary classification with:

\[
y \in \{-1, +1\}
\]

Prediction:

\[
f(x) = w^T x + b
\]

Squared hinge loss:

\[
L(y, f(x)) = \max(0, 1 - y f(x))^2
\]

---

## 3. Comparison with Standard Hinge Loss

Standard hinge:

\[
L = \max(0, 1 - y f(x))
\]

Squared hinge:

\[
L = \max(0, 1 - y f(x))^2
\]

Key difference:

- Hinge: linear penalty
- Squared hinge: quadratic penalty

---

## 4. Behavior by Region

### Case 1: Confident and Correct

\[
y f(x) \ge 1
\]

Loss = 0

---

### Case 2: Margin Violation

\[
y f(x) < 1
\]

Loss increases quadratically as violation increases.

More severe penalty than standard hinge.

---

## 5. Geometric Interpretation

Both hinge and squared hinge:

- Enforce a margin of 1.
- Penalize points inside the margin.

But squared hinge:

- Penalizes deeper violations more aggressively.
- Pushes misclassified points harder.

---

## 6. Optimization Perspective

Standard hinge:
- Piecewise linear
- Not differentiable at margin

Squared hinge:
- Still non-differentiable at boundary
- But smoother within violation region
- Gradient grows with violation

Gradient (if violation exists):

\[
\frac{\partial L}{\partial w}
=
-2(1 - y f(x)) y x
\]

Large violations â†’ large gradients.

---

## 7. SVM Objective with Squared Hinge

\[
\min_{w} \frac{1}{2} ||w||^2 + C \sum \max(0, 1 - y_i f(x_i))^2
\]

Compared to standard hinge:

- Emphasizes correcting large errors.
- Stronger regularization effect on hard examples.

---

## 8. Hinge vs Squared Hinge

| Aspect | Hinge | Squared Hinge |
|----------|--------|----------------|
| Penalty growth | Linear | Quadratic |
| Sensitivity to outliers | Moderate | Higher |
| Optimization smoothness | Less smooth | Slightly smoother |
| Gradient magnitude | Constant | Scales with violation |

Squared hinge behaves more like squared error in regression.

---

## 9. Biasâ€“Variance Perspective

Standard hinge:
- More robust to extreme violations.

Squared hinge:
- Stronger correction.
- May increase sensitivity to noisy outliers.

Choice affects biasâ€“variance tradeoff.

ðŸ”— Related:
- [[Biasâ€“Variance Tradeoff]]

---

## 10. Relationship to L2 Regularization

Squared hinge pairs naturally with:

- L2 regularization

Why?
Both penalize quadratically.

Standard hinge is often paired with:
- L1 or L2 regularization.

ðŸ”— Related:
- [[Regularization â€” MOC]]

---

## 11. When Squared Hinge Is Useful

- When large violations must be corrected strongly.
- When smoother gradients are preferred.
- When using gradient-based optimization.

---

## 12. When Itâ€™s Risky

- Noisy labels.
- Heavy outliers.
- Imbalanced datasets.

Quadratic growth amplifies extreme errors.

---

## 13. Relationship to Other Concepts

Squared Hinge Loss connects strongly to:

- [[Hinge Loss]]
- [[Support Vector Machine]]
- [[Margin-Based Losses]]
- [[Regularization â€” MOC]]
- [[Biasâ€“Variance Tradeoff]]

---

## 14. Why Squared Hinge Matters

Squared hinge teaches:

- How modifying loss curvature changes optimization behavior.
- Tradeoff between robustness and aggressiveness.
- How margin violations can be penalized differently.

It is a refinement of:
> Maximum margin learning.
