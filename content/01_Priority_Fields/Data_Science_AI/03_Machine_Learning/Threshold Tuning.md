---
type: concept
domain: machine-learning
category: classification
status: evergreen
---

# Threshold Tuning

> Threshold tuning is the process of adjusting the **decision threshold** that converts predicted probabilities into class labels in order to optimize business or performance objectives.

---

## 1. Core Idea

Most classifiers output **probabilities**, not class labels.

Default rule:
- Predict class = 1 if probability â‰¥ **0.5**

Threshold tuning asks:
> *Is 0.5 really the right cutoff for this problem?*

Often, it is not.

---

## 2. Why Threshold Tuning Is Necessary

Using a fixed 0.5 threshold assumes:
- Balanced classes
- Equal cost of false positives and false negatives

In real-world problems, these assumptions rarely hold.

ğŸ”— Related:
- [[Class Imbalance]]
- [[Cost-Sensitive Learning]]

---

## 3. Threshold vs Model Training

Important distinction:

- **Training** â†’ learns probabilities
- **Threshold tuning** â†’ converts probabilities into decisions

Threshold tuning:
- Does NOT change model weights
- Changes prediction behavior

It is a **post-training optimization**.

---

## 4. Effect of Threshold on Metrics

Changing the threshold directly affects:

- Precision
- Recall
- F1-score
- False Positive Rate
- False Negative Rate

| Threshold â†‘ | Precision | Recall |
|-----------|-----------|--------|
| Increase | â†‘ | â†“ |
| Decrease | â†“ | â†‘ |

ğŸ”— Related:
- [[Classification Metrics â€” MOC]]
- [[Confusion Matrix]]

---

## 5. Threshold Tuning and ROC / PR Curves

Threshold tuning corresponds to **moving along a curve**:

- ROC curve â†’ trade-off between TPR and FPR
- Precisionâ€“Recall curve â†’ trade-off between precision and recall

Each point on the curve = one threshold.

ğŸ”— Related:
- [[ROC Curve]]
- [[Precision Recall Curve]]

---

## 6. Common Threshold Selection Strategies

---

### 6.1 Metric Optimization

Choose threshold that:
- Maximizes F1-score
- Maximizes recall at fixed precision
- Minimizes cost function

---

### 6.2 Business Cost Optimization

Define a cost matrix:

- Cost(FP)
- Cost(FN)

Select threshold minimizing **expected cost**.

ğŸ”— Related:
- [[Cost-Sensitive Learning]]

---

### 6.3 Constraint-Based Tuning

Examples:
- Recall â‰¥ 95%
- False Positive Rate â‰¤ 1%

Useful in:
- Medical diagnosis
- Fraud detection

---

## 7. Threshold Tuning and Class Imbalance

In imbalanced datasets:
- Default threshold favors majority class
- Minority recall collapses

Threshold tuning is often **mandatory**.

ğŸ”— Related:
- [[Class Imbalance]]

---

## 8. Threshold Tuning and Probability Calibration

Threshold tuning assumes probabilities are meaningful.

Poor calibration â†’ unstable thresholds.

Calibration may be required first.

ğŸ”— Related:
- [[Probability Calibration]]

---

## 9. Threshold Tuning and Data Leakage âš ï¸

âŒ Wrong:
- Tuning threshold on test set

âœ… Correct:
1. Tune threshold on validation set
2. Lock threshold
3. Evaluate once on test set

ğŸ”— Related:
- [[Data Leakage]]
- [[Trainâ€“Test Split]]

---

## 10. Algorithm Compatibility

Threshold tuning applies to:
- Logistic Regression
- Naive Bayes
- Tree ensembles
- Boosting models
- Neural networks

Not applicable to:
- Hard-margin classifiers without probabilities (unless calibrated)

ğŸ”— Related:
- [[Probability Calibration]]

---

## 11. Common Mistakes

- âŒ Using accuracy to tune threshold  
- âŒ Ignoring business costs  
- âŒ Tuning on test set  
- âŒ Assuming threshold is model-specific  

Thresholds are **problem-specific**, not model-specific.

---

## 12. When Threshold Tuning Helps Most

- Imbalanced classification
- High-stakes decisions
- Cost-sensitive problems
- Precisionâ€“recall trade-offs

---

## 13. When Threshold Tuning Matters Less

- Balanced datasets
- Symmetric error costs
- Exploratory modeling

---

## 14. Why Threshold Tuning Matters

Threshold tuning explains:
- Why accuracy can be misleading
- Why the same model behaves differently in production
- Why business metrics outperform ML metrics
- Why probability outputs are powerful

It is the **final bridge from model to decision**.

---

## Usage Notes

- Link this note from:
  - Class Imbalance
  - Classification Metrics â€” MOC
  - Probability Calibration
  - Model Selection
- Treat threshold as a tunable hyperparameter (but post-training)
