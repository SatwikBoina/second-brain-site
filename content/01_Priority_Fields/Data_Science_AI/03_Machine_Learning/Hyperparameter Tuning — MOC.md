# üìå MOC ‚Äî Hyperparameter Tuning

> Central hub for **Hyperparameter Tuning** in Machine Learning.
> Hyperparameter tuning is the process of selecting values for model settings that control **learning behavior, model complexity, and generalization**, but are **not learned from data directly**.

---

## 1. Core Idea

Hyperparameters define *how a model learns*, not *what it learns*.

They control:
- Model capacity
- Regularization strength
- Optimization dynamics
- Decision boundaries

Poor hyperparameters can ruin a good model.  
Good hyperparameters can rescue a weak one.

---

## 2. Parameters vs Hyperparameters

| Aspect | Parameters | Hyperparameters |
|----|----|----|
| Learned from data | ‚úÖ | ‚ùå |
| Optimized during training | ‚úÖ | ‚ùå |
| Examples | Weights, coefficients | Learning rate, depth, C |

Hyperparameters are **chosen**, not learned.

---

## 3. Why Hyperparameter Tuning Matters

Hyperparameter tuning affects:
- Bias‚Äìvariance tradeoff
- Overfitting vs underfitting
- Training stability
- Final model performance
- Reproducibility

In practice:
> Performance differences often come more from tuning than model choice.

---

## 4. Common Hyperparameters Across Models

### Model Complexity
- Tree depth
- Number of estimators
- Number of layers / neurons

### Regularization
- L1 / L2 strength
- Dropout rate
- Early stopping patience

### Optimization
- Learning rate
- Batch size
- Optimizer choice

üîó Related:
- [[Model Complexity]]
- [[Regularization ‚Äî MOC]]

---

## 5. Hyperparameter Sensitivity

Not all hyperparameters matter equally.

- Some have **large performance impact**
- Some are highly **sensitive**
- Some barely matter

Understanding sensitivity determines **tuning strategy**.

üîó Related:
- [[Hyperparameter Sensitivity]]

---

## 6. Hyperparameter Search Strategies

---

### 6.1 Grid Search

- Exhaustive search over fixed grid
- Simple, but inefficient

üîó Related:
- [[Grid Search]]

---

### 6.2 Random Search

- Random sampling from distributions
- Much more efficient in high dimensions

üîó Related:
- [[Random Search]]

---

### 6.3 Bayesian Optimization

- Learns from previous trials
- Sample-efficient
- Expensive but powerful

üîó Related:
- [[Bayesian Optimization]]

---

### 6.4 Early-Stopping‚ÄìAware Methods

- Stop bad configurations early
- Allocate budget adaptively

Used heavily in deep learning.

üîó Related:
- [[Early Stopping]]

---

## 7. Validation Strategy for Tuning

Hyperparameter tuning must be done with **proper validation**.

Correct flow:
1. Train‚Äìtest split
2. Hyperparameter tuning on training (via CV)
3. Final evaluation on test set

Never tune on the test set.

üîó Related:
- [[Cross Validation]]
- [[Data Leakage]]

---

## 8. Hyperparameter Tuning and Bias‚ÄìVariance

- Strong regularization ‚Üí higher bias, lower variance
- Weak regularization ‚Üí lower bias, higher variance

Tuning is the *mechanism* to navigate this tradeoff.

üîó Related:
- [[Bias‚ÄìVariance Tradeoff]]
- [[Overfitting vs Underfitting]]

---

## 9. Model-Specific Tuning Difficulty

| Model | Tuning Difficulty |
|----|------------------|
| Linear Models | Low |
| Logistic Regression | Low |
| Decision Trees | Medium |
| Random Forest | Medium |
| Gradient Boosting | High |
| XGBoost / LightGBM | Very High |
| Neural Networks | Very High |

Complex models demand **careful tuning**.

---

## 10. Hyperparameter Tuning and Loss Functions

Tuning optimizes performance **relative to a loss / metric**.

Changing the loss:
- Changes the optimal hyperparameters
- Changes model behavior

üîó Related:
- [[Regression Loss Functions ‚Äî MOC]]
- [[Classification Loss Functions ‚Äî MOC]]

---

## 11. Hyperparameter Tuning and Compute Budget

Tuning is a **resource allocation problem**.

Trade-offs:
- Fewer trials √ó better strategy
- More trials √ó simpler strategy

Efficiency matters more than exhaustiveness.

---

## 12. Common Failure Modes

- ‚ùå Tuning too many parameters at once  
- ‚ùå Overfitting to validation set  
- ‚ùå Ignoring variance across folds  
- ‚ùå Blindly copying defaults from tutorials  
- ‚ùå Treating tuning as an afterthought  

Tuning is a **design step**, not cleanup.

---

## 13. Practical Tuning Heuristics

- Start with simple models
- Tune the most sensitive parameters first
- Use random search before Bayesian methods
- Prefer robust plateaus over sharp optima
- Track both mean and variance of metrics

---

## 14. Hyperparameter Tuning in Production

Production considerations:
- Stability > peak performance
- Reproducibility
- Monitoring drift
- Re-tuning schedules

A fragile model is a liability.

---

## 15. Relationship to Other Concepts

Hyperparameter tuning connects strongly to:

- [[Model Selection]]
- [[Evaluation Metrics ‚Äî MOC]]
- [[Neural Networks ‚Äî MOC]]
- [[Ensemble Learning ‚Äî MOC]]
- [[Probability Calibration]]

---

## 16. Why Hyperparameter Tuning Matters

Hyperparameter tuning explains:

- Why defaults rarely work best
- Why two identical models behave differently
- Why ‚Äúbest CV score‚Äù often fails in production
- Why robustness beats leaderboard chasing

It is the **control system of machine learning**.

---

## Usage Rules

- This MOC is a navigation + reasoning hub
- Search strategies live in separate notes
- Always pair tuning with proper validation
- Prefer understanding sensitivity over brute force
