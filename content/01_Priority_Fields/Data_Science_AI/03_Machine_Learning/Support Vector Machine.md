---
type: algorithm
domain: machine-learning
paradigm: supervised
problem-type: classification
model-family: kernel-methods
status: evergreen
---

# Support Vector Machine (SVM)

> A supervised learning algorithm that finds the optimal separating hyperplane by maximizing the margin between classes, optionally using kernels to model non-linear boundaries.

---

## 1. Core Idea

Support Vector Machine classifies data by finding a **decision boundary with the maximum margin** between classes.

Only a subset of points â€” the **support vectors** â€” define this boundary.

ğŸ”— Model family:
- [[Kernel Methods â€” MOC]]

---

## 2. Intuition

Instead of just separating classes, SVM asks:

> â€œWhat is the *most confident* separation I can make?â€

A larger margin means better generalization.

---

## 3. Analogy

Imagine placing a **fence** between two groups:
- You want it as far as possible from both sides
- Only the closest people to the fence matter
- Everyone else is irrelevant

Those closest people are the *support vectors*.

---

## 4. Mathematical Perspective

SVM solves a constrained optimization problem:

- Maximizes margin (â€–wâ€–â»Â¹)
- Penalizes misclassification via slack variables
- Has a convex objective â†’ global optimum guaranteed

ğŸ”— Concepts:
- [[Margin Maximization]]
- [[Convex Optimization]]
- [[Lagrangian Duality]]

---

## 5. Loss Function

SVM uses **hinge loss**:

- Correct classification beyond margin â†’ no loss
- Incorrect or weakly correct â†’ linear penalty

ğŸ”— See:
- [[Hinge Loss]]
- [[Loss Functions â€” MOC]]

---

## 6. Hard Margin vs Soft Margin

- **Hard Margin**  
  â†’ No misclassification allowed  
  â†’ Works only for perfectly separable data  

- **Soft Margin**  
  â†’ Allows violations  
  â†’ Controlled by regularization parameter `C`

ğŸ”— Concepts:
- [[Soft Margin SVM]]
- [[Regularization â€” MOC]]

---

## 7. Kernel Trick

SVM uses kernels to create **non-linear decision boundaries**:

- Linear
- Polynomial
- RBF (Gaussian)
- Sigmoid

ğŸ”— Concepts:
- [[Kernel Trick]]
- [[Reproducing Kernel Hilbert Space]]

---

## 8. Support Vectors

- Data points closest to the margin
- Define the decision boundary
- Model complexity depends on their number

ğŸ”— See:
- [[Support Vectors]]

---

## 9. Biasâ€“Variance Tradeoff

- Large margin â†’ higher bias, lower variance
- Small margin â†’ lower bias, higher variance
- Controlled by `C` and kernel parameters

ğŸ”— Links:
- [[Biasâ€“Variance Tradeoff]]

---

## 10. Assumptions

SVM assumes:

- Classes are separable in some feature space
- Kernel captures similarity correctly
- Sensitive to noise and overlapping classes

ğŸ”— Contrast:
- [[Tree-Based Models â€” MOC]]
- [[Linear Models â€” MOC]]

---

## 11. Hyperparameters (Critical)

Key hyperparameters include:

- C (regularization strength)
- Kernel type
- Gamma (for RBF kernel)
- Degree (for polynomial kernel)

ğŸ”— See:
- [[Hyperparameter Tuning â€” MOC]]

---

## 12. Feature Scaling Requirement

SVM is **extremely sensitive to feature scale**:

- Mandatory standardization
- Distance-based optimization depends on scale

ğŸ”— Links:
- [[Feature Scaling]]
- [[Standardization]]

---

## 13. Evaluation Metrics

Uses standard classification metrics:

- Accuracy
- Precision / Recall
- F1-score
- ROCâ€“AUC

ğŸ”— See:
- [[Classification Metrics â€” MOC]]

---

## 14. Interpretability

- Linear SVM â†’ interpretable coefficients
- Kernel SVM â†’ low interpretability
- Support vectors offer limited insight

ğŸ”— Links:
- [[Model Interpretability]]

---

## 15. Computational Characteristics

- Training complexity grows quickly with data size
- Memory intensive
- Not ideal for very large datasets

ğŸ”— Concepts:
- [[Computational Complexity]]

---

## 16. When SVM Works Well

- Small to medium datasets
- Clear margin between classes
- High-dimensional feature spaces (e.g. text)

---

## 17. When It Fails

- Very large datasets
- Noisy, overlapping classes
- Poor kernel or hyperparameter choice

---

## 18. Relationship to Other Models

- Classification counterpart of SVR
- Kernel-based alternative to Logistic Regression

ğŸ”— Related:
- [[Support Vector Regression]]
- [[Logistic Regression]]
- [[Kernel Ridge Regression]]

---

## 19. Position in the ML Landscape

- Strong theoretical foundations
- Geometry-based classifier
- Common in academia and niche production use
