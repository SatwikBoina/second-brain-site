- **Date of Entry:** 2026-01-31  
- **Tags:** #statistics #probability  


## One-line idea
As the sample size increases, the sample average converges to the expected value.

---

## The core problem

Random outcomes fluctuate heavily in small samples.

A few observations can be:

- misleading  
- unstable  
- far from the true average  

LLN explains what happens as we collect more data.

---

## What the law states

When a random process is repeated many times:


The convergence happens **because randomness averages out**.

---

## Conceptual view

Few samples → unstable average  
Many samples → stable average  
Infinite samples → expected value
![[lln.png]]

---

## What LLN does NOT say

LLN does NOT guarantee:

- individual outcomes become predictable  
- randomness disappears  
- small samples are accurate  

Randomness always exists.

Only averages stabilize.

---

## Why LLN matters

LLN is the reason:

- sample means estimate population means  
- surveys work  
- experiments converge  
- learning from data is possible  

Without LLN, statistics would collapse.

---

## Connection to inference

Statistical inference relies on:

- estimating population parameters  
- using sample statistics  

LLN explains why this estimation improves with more data.

---

## In short

> The Law of Large Numbers explains why more data leads to better estimates.

---
